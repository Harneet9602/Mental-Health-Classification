{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14638983,"sourceType":"datasetVersion","datasetId":9351524}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================\n# 0. INSTALL & LOGIN\n# ==============================\n!pip install transformers==4.36.2 accelerate==0.25.0 huggingface_hub==0.19.4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================\n# 1. IMPORTS & SETUP\n# ==============================\nimport re\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    AdamW,\n    get_linear_schedule_with_warmup\n)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\n\n# ==============================\n# 2. LOAD DATA\n# ==============================\ndf = pd.read_csv(\"/kaggle/input/mental-health/compressed_data.csv\")\n\ndf = df.rename(columns={\"statement\": \"text\", \"status\": \"label\"})\ndf = df.dropna(subset=[\"text\", \"label\"])\ndf = df.drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n\nlabel_names = sorted(df[\"label\"].unique())\nlabel2id = {l: i for i, l in enumerate(label_names)}\nid2label = {i: l for l, i in label2id.items()}\n\ndf[\"label_id\"] = df[\"label\"].map(label2id)\n\nprint(\"Classes:\", label2id)\nprint(\"Dataset shape:\", df.shape)\n\n\n# ==============================\n# 3. MINIMAL CLEANING (SAME AS YOUR CODE)\n# ==============================\ndef minimal_clean(text):\n    text = str(text)\n    text = re.sub(r\"http\\S+|www\\S+|@\\w+\", \"\", text)\n    text = re.sub(r\"<.*?>\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\ndf[\"text\"] = df[\"text\"].apply(minimal_clean)\n\n\n# ==============================\n# 4. TRAIN / TEST SPLIT (STRATIFIED)\n# ==============================\ntrain_df, test_df = train_test_split(\n    df,\n    test_size=0.15,\n    stratify=df[\"label_id\"],\n    random_state=SEED\n)\n\n\n# ==============================\n# 5. TOKENIZER & DATASET\n# ==============================\nMODEL_BASE = \"mental/mental-bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_BASE)\n\nclass MentalDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.encodings = tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding=True,\n            max_length=256\n        )\n        self.labels = labels.tolist()\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n        item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\ntrain_ds = MentalDataset(train_df[\"text\"], train_df[\"label_id\"])\ntest_ds  = MentalDataset(test_df[\"text\"], test_df[\"label_id\"])\n\ntrain_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\ntest_loader  = DataLoader(test_ds, batch_size=32)\n\n\n# ==============================\n# 6. MODEL\n# ==============================\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_BASE,\n    num_labels=len(label2id),\n    id2label=id2label,\n    label2id=label2id\n).to(device)\n\n\n# ==============================\n# 7. CLASS WEIGHTED LOSS\n# ==============================\nclass_weights = compute_class_weight(\n    class_weight=\"balanced\",\n    classes=np.unique(train_df[\"label_id\"]),\n    y=train_df[\"label_id\"]\n)\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n\n# ==============================\n# 8. OPTIMIZER & SCHEDULER\n# ==============================\noptimizer = AdamW(model.parameters(), lr=2e-5)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=len(train_loader) * 3\n)\n\n\n# ==============================\n# 9. STAGE 1: FREEZE BACKBONE (WARMUP)\n# ==============================\nfor param in model.base_model.parameters():\n    param.requires_grad = False\n\nprint(\"Backbone frozen — warmup training\")\n\nfor epoch in range(1):\n    model.train()\n    total_loss = 0\n\n    for batch in train_loader:\n        optimizer.zero_grad()\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        outputs = model(**batch)\n        loss = criterion(outputs.logits, batch[\"labels\"])\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        total_loss += loss.item()\n\n    print(f\"[Warmup] Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")\n\n# ==============================\n# ⭐ SAVE CHECKPOINT AFTER WARMUP\n# ==============================\nmodel.save_pretrained(\"mentalbert_warmup\")\ntokenizer.save_pretrained(\"mentalbert_warmup\")\nprint(\"✅ Warmup checkpoint saved\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================\n# 10. STAGE 2: UNFREEZE & FINETUNE\n# ==============================\nfor param in model.base_model.parameters():\n    param.requires_grad = True\n\nprint(\"Backbone unfrozen — full fine-tuning\")\n\nfor epoch in range(2):\n    model.train()\n    total_loss = 0\n\n    for batch in train_loader:\n        optimizer.zero_grad()\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        outputs = model(**batch)\n        loss = criterion(outputs.logits, batch[\"labels\"])\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        total_loss += loss.item()\n\n    print(f\"[Finetune] Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")\n\n\n# ==============================\n# 11. EVALUATION\n# ==============================\nmodel.eval()\ny_true, y_pred = [], []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        preds = torch.argmax(outputs.logits, dim=1)\n\n        y_true.extend(batch[\"labels\"].cpu().numpy())\n        y_pred.extend(preds.cpu().numpy())\n\nprint(classification_report(y_true, y_pred, target_names=label_names))\n\n\n# ==============================\n# 12. SAVE & PUSH MODEL\n# ==============================\nMODEL_NAME = \"mentalbert-mental-health\"\n\nmodel.save_pretrained(MODEL_NAME)\ntokenizer.save_pretrained(MODEL_NAME)\n\nfrom huggingface_hub import HfApi\napi = HfApi()\napi.create_repo(repo_id=MODEL_NAME, exist_ok=True)\napi.upload_folder(folder_path=MODEL_NAME, repo_id=MODEL_NAME)\n\nprint(\"✅ Model trained, evaluated, and pushed to Hugging Face\")","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-27T15:33:42.157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
