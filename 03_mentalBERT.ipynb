{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2025-11-22T09:58:04.299405Z",
     "iopub.status.busy": "2025-11-22T09:58:04.299233Z",
     "iopub.status.idle": "2025-11-22T09:58:23.009233Z",
     "shell.execute_reply": "2025-11-22T09:58:23.008378Z",
     "shell.execute_reply.started": "2025-11-22T09:58:04.299388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.53.3\n",
      "    Uninstalling transformers-4.53.3:\n",
      "      Successfully uninstalled transformers-4.53.3\n",
      "Successfully installed tokenizers-0.22.1 transformers-4.57.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Local Inference on GPU \n",
    "Model page: https://huggingface.co/mental/mental-bert-base-uncased\n",
    "\n",
    "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/mental/mental-bert-base-uncased)\n",
    "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. ü§ó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2025-11-24T09:34:11.357253Z",
     "iopub.status.busy": "2025-11-24T09:34:11.357001Z",
     "iopub.status.idle": "2025-11-24T09:34:11.717358Z",
     "shell.execute_reply": "2025-11-24T09:34:11.716193Z",
     "shell.execute_reply.started": "2025-11-24T09:34:11.357227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46b8bd0a63c4363878610f8c34c2825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2025-11-24T09:35:11.462312Z",
     "iopub.status.busy": "2025-11-24T09:35:11.461950Z",
     "iopub.status.idle": "2025-11-24T09:35:21.401206Z",
     "shell.execute_reply": "2025-11-24T09:35:21.400351Z",
     "shell.execute_reply.started": "2025-11-24T09:35:11.462287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7f004fbfde4d36a445afc360cc32e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/639 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869db97beac4445d81d0fa82f6fb80ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9739e468d9ca4020b81ace1f0144647e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/321 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06a632e3fc242f1b83deb5ad3d6aa28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791ef20aa56640ab81d0658e669b454b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5775918645c44ebbbdd226faaa081106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32680be9325444eac4c32e15d1ae48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=\"mental/mental-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2025-11-24T09:35:21.402758Z",
     "iopub.status.busy": "2025-11-24T09:35:21.402470Z",
     "iopub.status.idle": "2025-11-24T09:35:23.049998Z",
     "shell.execute_reply": "2025-11-24T09:35:23.049193Z",
     "shell.execute_reply.started": "2025-11-24T09:35:21.402739Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mental/mental-bert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"mental/mental-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T09:35:23.050967Z",
     "iopub.status.busy": "2025-11-24T09:35:23.050779Z",
     "iopub.status.idle": "2025-11-24T09:35:24.622720Z",
     "shell.execute_reply": "2025-11-24T09:35:24.621874Z",
     "shell.execute_reply.started": "2025-11-24T09:35:23.050953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported. Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# CELL 1: CLEAN IMPORTS & SETUP\n",
    "# ===============================================================================\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Hugging Face & Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Random Seed (For Reproducibility)\n",
    "SEED = 42\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(SEED)\n",
    "\n",
    "print(f\"‚úì Libraries imported. Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T09:35:24.624780Z",
     "iopub.status.busy": "2025-11-24T09:35:24.624220Z",
     "iopub.status.idle": "2025-11-24T09:35:28.099872Z",
     "shell.execute_reply": "2025-11-24T09:35:28.099071Z",
     "shell.execute_reply.started": "2025-11-24T09:35:24.624760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data Loaded. Shape: (51073, 3)\n",
      "‚úì Classes: {'Anxiety': 0, 'Bipolar': 1, 'Depression': 2, 'Normal': 3, 'Personality disorder': 4, 'Stress': 5, 'Suicidal': 6}\n",
      "‚öñÔ∏è Class Weights calculated: tensor([2.0175, 2.9171, 0.4836, 0.4549, 8.1494, 3.1820, 0.6857],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 2. CONFIGURATION\n",
    "DATA_PATH = \"/kaggle/input/mental-health/Combined Data.csv\"\n",
    "OUT_DIR = \"distilbert-mental-health\"\n",
    "BATCH_SIZE = 16          # DistilBERT is small, so 16 usually fits on P100 GPU\n",
    "EPOCHS = 3\n",
    "LR = 2e-5                # Standard learning rate for Transformers\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(SEED)\n",
    "\n",
    "\n",
    "# Load data from Kaggle dataset \n",
    "filepath = \"/kaggle/input/mental-health/Combined Data.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Standardize columns\n",
    "if \"Unnamed: 0\" in df.columns:\n",
    "    df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "df = df.rename(columns={\"statement\": \"text\", \"status\": \"label_name\"})\n",
    "\n",
    "# Drop Nulls/Duplicates\n",
    "df = df.dropna(subset=[\"text\", \"label_name\"]).drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n",
    "\n",
    "# Encode Labels (String -> Integer) AFTER filtering\n",
    "label_values = sorted(df[\"label_name\"].unique())\n",
    "label2id = {name: i for i, name in enumerate(label_values)}\n",
    "id2label = {i: name for name, i in label2id.items()}\n",
    "df[\"label\"] = df[\"label_name\"].map(label2id)\n",
    "\n",
    "print(f\"‚úì Data Loaded. Shape: {df.shape}\")\n",
    "print(f\"‚úì Classes: {label2id}\")\n",
    "\n",
    "\n",
    "# Minimal Cleaning Function (Best for Transformers)\n",
    "def minimal_clean(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|@\\w+\", \"\", text)  # Remove URLs/Mentions\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)                 # Remove HTML\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()          # Remove double spaces\n",
    "    return text\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(minimal_clean)\n",
    "\n",
    "# 4. SPLIT & TOKENIZE\n",
    "# Stratified split ensures equal distribution of classes in Train/Test\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, stratify=df[\"label\"], random_state=SEED)\n",
    "\n",
    "hf_train = Dataset.from_pandas(train_df[[\"text\", \"label\"]].reset_index(drop=True))\n",
    "hf_test = Dataset.from_pandas(test_df[[\"text\", \"label\"]].reset_index(drop=True))\n",
    "dataset = DatasetDict({\"train\": hf_train, \"test\": hf_test})\n",
    "\n",
    "# 5. COMPUTE CLASS WEIGHTS (Handle Imbalance)\n",
    "labels = train_df[\"label\"].values\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"‚öñÔ∏è Class Weights calculated: {class_weights}\")\n",
    "\n",
    "# 6. CUSTOM TRAINER (Fixed for newer Transformers versions)\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # We add **kwargs above to catch 'num_items_in_batch' or any other new args\n",
    "        \n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Calculate loss with our custom class weights\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T09:35:28.101052Z",
     "iopub.status.busy": "2025-11-24T09:35:28.100758Z",
     "iopub.status.idle": "2025-11-24T10:35:39.411386Z",
     "shell.execute_reply": "2025-11-24T10:35:39.410079Z",
     "shell.execute_reply.started": "2025-11-24T09:35:28.101022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================\n",
      "ü•ä TRAINING ROUND: MentalBERT\n",
      "========================================\n",
      ">>> Tokenizing for MentalBERT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2552b99c37f495a854f94a8c7734294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/43412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f34475531347379c03cee3929dfc49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7661 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùÑÔ∏è Freezing backbone for warmup...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2714' max='2714' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2714/2714 05:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.953200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.781700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.774800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.754900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Unfreezing for full training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8142' max='8142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8142/8142 53:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.500066</td>\n",
       "      <td>0.819997</td>\n",
       "      <td>0.785861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>0.575486</td>\n",
       "      <td>0.834486</td>\n",
       "      <td>0.806956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.654026</td>\n",
       "      <td>0.833442</td>\n",
       "      <td>0.808955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä MentalBERT Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Anxiety       0.87      0.88      0.88       543\n",
      "             Bipolar       0.86      0.82      0.84       375\n",
      "          Depression       0.83      0.72      0.77      2263\n",
      "              Normal       0.96      0.95      0.96      2406\n",
      "Personality disorder       0.71      0.72      0.72       134\n",
      "              Stress       0.70      0.81      0.75       344\n",
      "            Suicidal       0.69      0.81      0.75      1596\n",
      "\n",
      "            accuracy                           0.83      7661\n",
      "           macro avg       0.80      0.82      0.81      7661\n",
      "        weighted avg       0.84      0.83      0.83      7661\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "confusion_matrix() got an unexpected keyword argument 'palette'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48/322242867.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# Plot Confusion Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'viridis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mdisp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Blues'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticks_rotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: confusion_matrix() got an unexpected keyword argument 'palette'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0UAAAKZCAYAAAB3DIBVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAkEklEQVR4nO3df2zX9Z3A8RctttXMVjyO8uPqON05t6ngQHrVGeOls8kMO/64jEMDhOg8J2fUZjfBH3TOG+V2zpCcOCJz5/7xYDPTLIPguU6y7OyFjB+J5gDDGIOYtcDtbLm6UWg/98didx1F+Za2CK/HI/n+0bfv9/fz/pq36NPPt9/vuKIoigAAAEiq7GxvAAAA4GwSRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGolR9FPf/rTmDt3bkydOjXGjRsXL7/88geu2bJlS3z605+OysrK+NjHPhbPP//8MLYKAAAw8kqOop6enpgxY0asWbPmtOb/8pe/jNtuuy1uueWW2LlzZzzwwANx1113xSuvvFLyZgEAAEbauKIoimEvHjcuXnrppZg3b94p5zz00EOxcePGePPNNwfG/vZv/zbeeeed2Lx583AvDQAAMCLGj/YF2tvbo7GxcdBYU1NTPPDAA6dcc+zYsTh27NjAz/39/fGb3/wm/uRP/iTGjRs3WlsFAAA+5IqiiKNHj8bUqVOjrGxkPiJh1KOoo6MjamtrB43V1tZGd3d3/Pa3v40LL7zwpDWtra3x+OOPj/bWAACAc9TBgwfjz/7sz0bkuUY9ioZj+fLl0dzcPPBzV1dXXHbZZXHw4MGorq4+izsDAADOpu7u7qirq4uLL754xJ5z1KNo8uTJ0dnZOWiss7Mzqqurh7xLFBFRWVkZlZWVJ41XV1eLIgAAYER/rWbUv6eooaEh2traBo29+uqr0dDQMNqXBgAA+EAlR9H//u//xs6dO2Pnzp0R8fuP3N65c2ccOHAgIn7/1rdFixYNzL/nnnti37598ZWvfCV2794dzzzzTHzve9+LBx98cGReAQAAwBkoOYp+/vOfx3XXXRfXXXddREQ0NzfHddddFytWrIiIiF//+tcDgRQR8ed//uexcePGePXVV2PGjBnxzW9+M7797W9HU1PTCL0EAACA4Tuj7ykaK93d3VFTUxNdXV1+pwgAABIbjTYY9d8pAgAA+DATRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJDasKJozZo1MX369Kiqqor6+vrYunXr+85fvXp1fPzjH48LL7ww6urq4sEHH4zf/e53w9owAADASCo5ijZs2BDNzc3R0tIS27dvjxkzZkRTU1McOnRoyPkvvPBCLFu2LFpaWmLXrl3x3HPPxYYNG+Lhhx8+480DAACcqZKj6KmnnoovfvGLsWTJkvjkJz8Za9eujYsuuii+853vDDn/9ddfjxtvvDFuv/32mD59etx6662xYMGCD7y7BAAAMBZKiqLe3t7Ytm1bNDY2/uEJysqisbEx2tvbh1xzww03xLZt2wYiaN++fbFp06b43Oc+d8rrHDt2LLq7uwc9AAAARsP4UiYfOXIk+vr6ora2dtB4bW1t7N69e8g1t99+exw5ciQ+85nPRFEUceLEibjnnnve9+1zra2t8fjjj5eyNQAAgGEZ9U+f27JlS6xcuTKeeeaZ2L59e/zgBz+IjRs3xhNPPHHKNcuXL4+urq6Bx8GDB0d7mwAAQFIl3SmaOHFilJeXR2dn56Dxzs7OmDx58pBrHnvssVi4cGHcddddERFxzTXXRE9PT9x9993xyCOPRFnZyV1WWVkZlZWVpWwNAABgWEq6U1RRURGzZs2Ktra2gbH+/v5oa2uLhoaGIde8++67J4VPeXl5REQURVHqfgEAAEZUSXeKIiKam5tj8eLFMXv27JgzZ06sXr06enp6YsmSJRERsWjRopg2bVq0trZGRMTcuXPjqaeeiuuuuy7q6+tj79698dhjj8XcuXMH4ggAAOBsKTmK5s+fH4cPH44VK1ZER0dHzJw5MzZv3jzw4QsHDhwYdGfo0UcfjXHjxsWjjz4ab7/9dvzpn/5pzJ07N77+9a+P3KsAAAAYpnHFOfAetu7u7qipqYmurq6orq4+29sBAADOktFog1H/9DkAAIAPM1EEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqQ0ritasWRPTp0+PqqqqqK+vj61bt77v/HfeeSeWLl0aU6ZMicrKyrjyyitj06ZNw9owAADASBpf6oINGzZEc3NzrF27Nurr62P16tXR1NQUe/bsiUmTJp00v7e3Nz772c/GpEmT4sUXX4xp06bFr371q7jkkktGYv8AAABnZFxRFEUpC+rr6+P666+Pp59+OiIi+vv7o66uLu67775YtmzZSfPXrl0b//zP/xy7d++OCy64YFib7O7ujpqamujq6orq6uphPQcAAHDuG402KOntc729vbFt27ZobGz8wxOUlUVjY2O0t7cPueaHP/xhNDQ0xNKlS6O2tjauvvrqWLlyZfT19Z3ZzgEAAEZASW+fO3LkSPT19UVtbe2g8dra2ti9e/eQa/bt2xc/+clP4o477ohNmzbF3r174957743jx49HS0vLkGuOHTsWx44dG/i5u7u7lG0CAACctlH/9Ln+/v6YNGlSPPvsszFr1qyYP39+PPLII7F27dpTrmltbY2ampqBR11d3WhvEwAASKqkKJo4cWKUl5dHZ2fnoPHOzs6YPHnykGumTJkSV155ZZSXlw+MfeITn4iOjo7o7e0dcs3y5cujq6tr4HHw4MFStgkAAHDaSoqiioqKmDVrVrS1tQ2M9ff3R1tbWzQ0NAy55sYbb4y9e/dGf3//wNhbb70VU6ZMiYqKiiHXVFZWRnV19aAHAADAaCj57XPNzc2xbt26+O53vxu7du2KL33pS9HT0xNLliyJiIhFixbF8uXLB+Z/6Utfit/85jdx//33x1tvvRUbN26MlStXxtKlS0fuVQAAAAxTyd9TNH/+/Dh8+HCsWLEiOjo6YubMmbF58+aBD184cOBAlJX9obXq6urilVdeiQcffDCuvfbamDZtWtx///3x0EMPjdyrAAAAGKaSv6fobPA9RQAAQMSH4HuKAAAAzjeiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGqiCAAASE0UAQAAqYkiAAAgNVEEAACkJooAAIDURBEAAJCaKAIAAFITRQAAQGrDiqI1a9bE9OnTo6qqKurr62Pr1q2ntW79+vUxbty4mDdv3nAuCwAAMOJKjqINGzZEc3NztLS0xPbt22PGjBnR1NQUhw4det91+/fvjy9/+ctx0003DXuzAAAAI63kKHrqqafii1/8YixZsiQ++clPxtq1a+Oiiy6K73znO6dc09fXF3fccUc8/vjjcfnll5/RhgEAAEZSSVHU29sb27Zti8bGxj88QVlZNDY2Rnt7+ynXfe1rX4tJkybFnXfeeVrXOXbsWHR3dw96AAAAjIaSoujIkSPR19cXtbW1g8Zra2ujo6NjyDU/+9nP4rnnnot169ad9nVaW1ujpqZm4FFXV1fKNgEAAE7bqH763NGjR2PhwoWxbt26mDhx4mmvW758eXR1dQ08Dh48OIq7BAAAMhtfyuSJEydGeXl5dHZ2Dhrv7OyMyZMnnzT/F7/4Rezfvz/mzp07MNbf3//7C48fH3v27IkrrrjipHWVlZVRWVlZytYAAACGpaQ7RRUVFTFr1qxoa2sbGOvv74+2trZoaGg4af5VV10Vb7zxRuzcuXPg8fnPfz5uueWW2Llzp7fFAQAAZ11Jd4oiIpqbm2Px4sUxe/bsmDNnTqxevTp6enpiyZIlERGxaNGimDZtWrS2tkZVVVVcffXVg9ZfcsklEREnjQMAAJwNJUfR/Pnz4/Dhw7FixYro6OiImTNnxubNmwc+fOHAgQNRVjaqv6oEAAAwYsYVRVGc7U18kO7u7qipqYmurq6orq4+29sBAADOktFoA7d0AACA1EQRAACQmigCAABSE0UAAEBqoggAAEhNFAEAAKmJIgAAIDVRBAAApCaKAACA1EQRAACQmigCAABSE0UAAEBqoggAAEhNFAEAAKmJIgAAIDVRBAAApCaKAACA1EQRAACQmigCAABSE0UAAEBqoggAAEhNFAEAAKmJIgAAIDVRBAAApCaKAACA1EQRAACQmigCAABSE0UAAEBqoggAAEhNFAEAAKmJIgAAIDVRBAAApCaKAACA1EQRAACQmigCAABSE0UAAEBqoggAAEhNFAEAAKmJIgAAIDVRBAAApCaKAACA1EQRAACQmigCAABSE0UAAEBqoggAAEhNFAEAAKmJIgAAIDVRBAAApCaKAACA1EQRAACQmigCAABSE0UAAEBqoggAAEhNFAEAAKmJIgAAIDVRBAAApCaKAACA1EQRAACQmigCAABSE0UAAEBqoggAAEhNFAEAAKmJIgAAIDVRBAAApCaKAACA1EQRAACQmigCAABSE0UAAEBqoggAAEhNFAEAAKmJIgAAIDVRBAAApCaKAACA1EQRAACQmigCAABSE0UAAEBqoggAAEhNFAEAAKmJIgAAIDVRBAAApCaKAACA1EQRAACQmigCAABSE0UAAEBqoggAAEhNFAEAAKmJIgAAIDVRBAAApCaKAACA1EQRAACQmigCAABSE0UAAEBqoggAAEhNFAEAAKmJIgAAIDVRBAAApCaKAACA1EQRAACQmigCAABSE0UAAEBqoggAAEhNFAEAAKmJIgAAIDVRBAAApCaKAACA1EQRAACQmigCAABSE0UAAEBqoggAAEhNFAEAAKmJIgAAIDVRBAAApDasKFqzZk1Mnz49qqqqor6+PrZu3XrKuevWrYubbropJkyYEBMmTIjGxsb3nQ8AADCWSo6iDRs2RHNzc7S0tMT27dtjxowZ0dTUFIcOHRpy/pYtW2LBggXx2muvRXt7e9TV1cWtt94ab7/99hlvHgAA4EyNK4qiKGVBfX19XH/99fH0009HRER/f3/U1dXFfffdF8uWLfvA9X19fTFhwoR4+umnY9GiRad1ze7u7qipqYmurq6orq4uZbsAAMB5ZDTaoKQ7Rb29vbFt27ZobGz8wxOUlUVjY2O0t7ef1nO8++67cfz48bj00ktPOefYsWPR3d096AEAADAaSoqiI0eORF9fX9TW1g4ar62tjY6OjtN6joceeiimTp06KKz+WGtra9TU1Aw86urqStkmAADAaRvTT59btWpVrF+/Pl566aWoqqo65bzly5dHV1fXwOPgwYNjuEsAACCT8aVMnjhxYpSXl0dnZ+eg8c7Ozpg8efL7rn3yySdj1apV8eMf/ziuvfba951bWVkZlZWVpWwNAABgWEq6U1RRURGzZs2Ktra2gbH+/v5oa2uLhoaGU677xje+EU888URs3rw5Zs+ePfzdAgAAjLCS7hRFRDQ3N8fixYtj9uzZMWfOnFi9enX09PTEkiVLIiJi0aJFMW3atGhtbY2IiH/6p3+KFStWxAsvvBDTp08f+N2jj3zkI/GRj3xkBF8KAABA6UqOovnz58fhw4djxYoV0dHRETNnzozNmzcPfPjCgQMHoqzsDzegvvWtb0Vvb2/8zd/8zaDnaWlpia9+9atntnsAAIAzVPL3FJ0NvqcIAACI+BB8TxEAAMD5RhQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAasOKojVr1sT06dOjqqoq6uvrY+vWre87//vf/35cddVVUVVVFddcc01s2rRpWJsFAAAYaSVH0YYNG6K5uTlaWlpi+/btMWPGjGhqaopDhw4NOf/111+PBQsWxJ133hk7duyIefPmxbx58+LNN988480DAACcqXFFURSlLKivr4/rr78+nn766YiI6O/vj7q6urjvvvti2bJlJ82fP39+9PT0xI9+9KOBsb/8y7+MmTNnxtq1a0/rmt3d3VFTUxNdXV1RXV1dynYBAIDzyGi0wfhSJvf29sa2bdti+fLlA2NlZWXR2NgY7e3tQ65pb2+P5ubmQWNNTU3x8ssvn/I6x44di2PHjg383NXVFRG//xsAAADk9V4TlHhv532VFEVHjhyJvr6+qK2tHTReW1sbu3fvHnJNR0fHkPM7OjpOeZ3W1tZ4/PHHTxqvq6srZbsAAMB56r//+7+jpqZmRJ6rpCgaK8uXLx90d+mdd96Jj370o3HgwIERe+EwlO7u7qirq4uDBw96qyajylljrDhrjBVnjbHS1dUVl112WVx66aUj9pwlRdHEiROjvLw8Ojs7B413dnbG5MmTh1wzefLkkuZHRFRWVkZlZeVJ4zU1Nf4hY0xUV1c7a4wJZ42x4qwxVpw1xkpZ2ch9u1BJz1RRURGzZs2Ktra2gbH+/v5oa2uLhoaGIdc0NDQMmh8R8eqrr55yPgAAwFgq+e1zzc3NsXjx4pg9e3bMmTMnVq9eHT09PbFkyZKIiFi0aFFMmzYtWltbIyLi/vvvj5tvvjm++c1vxm233Rbr16+Pn//85/Hss8+O7CsBAAAYhpKjaP78+XH48OFYsWJFdHR0xMyZM2Pz5s0DH6Zw4MCBQbeybrjhhnjhhRfi0UcfjYcffjj+4i/+Il5++eW4+uqrT/ualZWV0dLSMuRb6mAkOWuMFWeNseKsMVacNcbKaJy1kr+nCAAA4Hwycr+dBAAAcA4SRQAAQGqiCAAASE0UAQAAqX1oomjNmjUxffr0qKqqivr6+ti6dev7zv/+978fV111VVRVVcU111wTmzZtGqOdcq4r5aytW7cubrrpppgwYUJMmDAhGhsbP/BswntK/XPtPevXr49x48bFvHnzRneDnDdKPWvvvPNOLF26NKZMmRKVlZVx5ZVX+vcop6XUs7Z69er4+Mc/HhdeeGHU1dXFgw8+GL/73e/GaLeci37605/G3LlzY+rUqTFu3Lh4+eWXP3DNli1b4tOf/nRUVlbGxz72sXj++edLvu6HIoo2bNgQzc3N0dLSEtu3b48ZM2ZEU1NTHDp0aMj5r7/+eixYsCDuvPPO2LFjR8ybNy/mzZsXb7755hjvnHNNqWdty5YtsWDBgnjttdeivb096urq4tZbb4233357jHfOuabUs/ae/fv3x5e//OW46aabxminnOtKPWu9vb3x2c9+Nvbv3x8vvvhi7NmzJ9atWxfTpk0b451zrin1rL3wwguxbNmyaGlpiV27dsVzzz0XGzZsiIcffniMd865pKenJ2bMmBFr1qw5rfm//OUv47bbbotbbrkldu7cGQ888EDcdddd8corr5R24eJDYM6cOcXSpUsHfu7r6yumTp1atLa2Djn/C1/4QnHbbbcNGquvry/+7u/+blT3ybmv1LP2x06cOFFcfPHFxXe/+93R2iLnieGctRMnThQ33HBD8e1vf7tYvHhx8dd//ddjsFPOdaWetW9961vF5ZdfXvT29o7VFjlPlHrWli5dWvzVX/3VoLHm5ubixhtvHNV9cv6IiOKll1563zlf+cpXik996lODxubPn180NTWVdK2zfqeot7c3tm3bFo2NjQNjZWVl0djYGO3t7UOuaW9vHzQ/IqKpqemU8yFieGftj7377rtx/PjxuPTSS0drm5wHhnvWvva1r8WkSZPizjvvHIttch4Yzln74Q9/GA0NDbF06dKora2Nq6++OlauXBl9fX1jtW3OQcM5azfccENs27Zt4C12+/bti02bNsXnPve5MdkzOYxUF4wfyU0Nx5EjR6Kvry9qa2sHjdfW1sbu3buHXNPR0THk/I6OjlHbJ+e+4Zy1P/bQQw/F1KlTT/qHD/6/4Zy1n/3sZ/Hcc8/Fzp07x2CHnC+Gc9b27dsXP/nJT+KOO+6ITZs2xd69e+Pee++N48ePR0tLy1hsm3PQcM7a7bffHkeOHInPfOYzURRFnDhxIu655x5vn2NEnaoLuru747e//W1ceOGFp/U8Z/1OEZwrVq1aFevXr4+XXnopqqqqzvZ2OI8cPXo0Fi5cGOvWrYuJEyee7e1wnuvv749JkybFs88+G7NmzYr58+fHI488EmvXrj3bW+M8s2XLlli5cmU888wzsX379vjBD34QGzdujCeeeOJsbw1OctbvFE2cODHKy8ujs7Nz0HhnZ2dMnjx5yDWTJ08uaT5EDO+svefJJ5+MVatWxY9//OO49tprR3ObnAdKPWu/+MUvYv/+/TF37tyBsf7+/oiIGD9+fOzZsyeuuOKK0d0056Th/Lk2ZcqUuOCCC6K8vHxg7BOf+ER0dHREb29vVFRUjOqeOTcN56w99thjsXDhwrjrrrsiIuKaa66Jnp6euPvuu+ORRx6JsjL/b54zd6ouqK6uPu27RBEfgjtFFRUVMWvWrGhraxsY6+/vj7a2tmhoaBhyTUNDw6D5ERGvvvrqKedDxPDOWkTEN77xjXjiiSdi8+bNMXv27LHYKue4Us/aVVddFW+88Ubs3Llz4PH5z39+4JN06urqxnL7nEOG8+fajTfeGHv37h0I74iIt956K6ZMmSKIOKXhnLV33333pPB5L8Z//zv0cOZGrAtK+wyI0bF+/fqisrKyeP7554v/+q//Ku6+++7ikksuKTo6OoqiKIqFCxcWy5YtG5j/H//xH8X48eOLJ598sti1a1fR0tJSXHDBBcUbb7xxtl4C54hSz9qqVauKioqK4sUXXyx+/etfDzyOHj16tl4C54hSz9of8+lznK5Sz9qBAweKiy++uPj7v//7Ys+ePcWPfvSjYtKkScU//uM/nq2XwDmi1LPW0tJSXHzxxcW//du/Ffv27Sv+/d//vbjiiiuKL3zhC2frJXAOOHr0aLFjx45ix44dRUQUTz31VLFjx47iV7/6VVEURbFs2bJi4cKFA/P37dtXXHTRRcU//MM/FLt27SrWrFlTlJeXF5s3by7puh+KKCqKoviXf/mX4rLLLisqKiqKOXPmFP/5n/858NduvvnmYvHixYPmf+973yuuvPLKoqKiovjUpz5VbNy4cYx3zLmqlLP20Y9+tIiIkx4tLS1jv3HOOaX+ufb/iSJKUepZe/3114v6+vqisrKyuPzyy4uvf/3rxYkTJ8Z415yLSjlrx48fL7761a8WV1xxRVFVVVXU1dUV9957b/E///M/Y79xzhmvvfbakP/t9d7ZWrx4cXHzzTeftGbmzJlFRUVFcfnllxf/+q//WvJ1xxWF+5cAAEBeZ/13igAAAM4mUQQAAKQmigAAgNREEQAAkJooAgAAUhNFAABAaqIIAABITRQBAACpiSIAACA1UQQAAKQmigAAgNREEQAAkNr/Absj/OP5p8uWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# ‚¨áÔ∏è PASTE YOUR HUGGING FACE TOKEN HERE ‚¨áÔ∏è\n",
    "HF_TOKEN = \"hf_********************************" \n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "FRIENDLY_NAME = \"MentalBERT\"\n",
    "MODEL_PATH    = \"mental/mental-bert-base-uncased\"\n",
    "\n",
    "print(f\"\\n\\n{'='*40}\")\n",
    "print(f\"ü•ä TRAINING ROUND: {FRIENDLY_NAME}\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "# 1. Tokenize\n",
    "print(f\">>> Tokenizing for {FRIENDLY_NAME}...\")\n",
    "# MentalBERT uses the same tokenizer logic as BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, token=HF_TOKEN)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=256)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 2. Load Model (With Token)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    token=HF_TOKEN # <--- Required for Gated Models\n",
    ")\n",
    "\n",
    "# 3. STAGE 1: WARMUP (Freeze Body)\n",
    "print(f\"‚ùÑÔ∏è Freezing backbone for warmup...\")\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "warmup_args = TrainingArguments(\n",
    "    output_dir=f\"comparison_{FRIENDLY_NAME}_warmup\",\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "Trainer(\n",
    "    model=model, args=warmup_args, \n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator\n",
    ").train()\n",
    "\n",
    "# 4. STAGE 2: FINE-TUNE (Unfreeze All)\n",
    "print(f\"üî• Unfreezing for full training...\")\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"comparison_{FRIENDLY_NAME}_final\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 5. EVALUATION\n",
    "print(f\"\\nüìä {FRIENDLY_NAME} Results:\")\n",
    "\n",
    "preds_output = trainer.predict(tokenized_datasets[\"test\"])\n",
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "y_true = preds_output.label_ids\n",
    "\n",
    "print(classification_report(y_true, y_preds, target_names=label_values))\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_true, y_preds, palette = 'viridis', labels=range(len(label_values)))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_values)\n",
    "disp.plot(cmap='Blues', ax=ax, xticks_rotation=45)\n",
    "plt.title(f\"Confusion Matrix: {FRIENDLY_NAME}\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Save Score\n",
    "metrics = trainer.evaluate()\n",
    "results_list.append({\n",
    "    \"Model\": FRIENDLY_NAME,\n",
    "    \"Accuracy\": metrics[\"eval_accuracy\"],\n",
    "    \"F1_Macro\": metrics[\"eval_f1_macro\"]\n",
    "})\n",
    "\n",
    "# Cleanup\n",
    "del model, trainer, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"‚úÖ {FRIENDLY_NAME} Finished & Cleared from Memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6426913,
     "sourceId": 10375532,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
