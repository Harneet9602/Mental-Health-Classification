{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14594461,"sourceType":"datasetVersion","datasetId":9322454}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nCAPSTONE PROJECT: MULTI-CLASS MENTAL HEALTH CLASSIFICATION\nComplete notebook with preprocessing, data loading, and all 6 transformer models\n\"\"\"\n\n# ===============================================================================\n# CELL 0: INSTALL REQUIRED PACKAGES\n# ===============================================================================\n\nimport subprocess\nimport sys\n\ndef install_packages():\n    \"\"\"Install required packages with compatible versions for Kaggle\"\"\"\n    \n    packages_to_install = [\n        (\"numpy\", \"numpy<2.0\"),\n        (\"scikit-learn\", \"scikit-learn>=1.0.0\"),\n        (\"pandas\", \"pandas\"),\n        (\"matplotlib\", \"matplotlib\"),\n        (\"seaborn\", \"seaborn\"),\n        (\"nltk\", \"nltk\"),\n        (\"torch\", \"torch\"),\n        (\"transformers\", \"transformers\"),\n        (\"datasets\", \"datasets\")\n    ]\n    \n    installed_count = 0\n    for package_name, package_spec in packages_to_install:\n        try:\n            subprocess.check_call(\n                [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package_spec],\n                stderr=subprocess.DEVNULL\n            )\n            print(f\"âœ“ {package_name}\")\n            installed_count += 1\n        except Exception as e:\n            print(f\"âš  {package_name} - {str(e)[:50]}\")\n    \n    print(f\"\\nâœ“ Installation complete! {installed_count}/{len(packages_to_install)} packages ready.\")\n    print(\"âš  Note: Some Kaggle pre-existing conflicts are normal and safe to ignore.\\n\")\n\ninstall_packages()","metadata":{"trusted":true,"editable":false,"execution":{"execution_failed":"2026-01-23T15:04:57.464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ===============================================================================\n# # FIX CUDNN COMPATIBILITY (KAGGLE P100)\n# # ===============================================================================\n\n# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# # Disable cuDNN to avoid version mismatch on Kaggle\n# tf.config.list_physical_devices('GPU')\n# tf.config.set_visible_devices([], 'GPU')\n\n# # Re-enable GPU with memory growth to prevent OOM\n# gpus = tf.config.list_physical_devices('GPU')\n# for gpu in gpus:\n#     try:\n#         tf.config.experimental.set_memory_growth(gpu, True)\n#     except RuntimeError as e:\n#         print(f\"GPU setup warning (safe to ignore): {e}\")\n\n# print(\"âœ“ GPU configured and cuDNN compatibility fixed!\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================================================================\n# CELL 1: CLEAN IMPORTS & SETUP\n# ===============================================================================\nimport os\nimport re\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\n\n# Hugging Face & Sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding,\n    TrainingArguments,\n    Trainer,\n    pipeline\n)\n\n# Settings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_colwidth', None)\n\n# Random Seed (For Reproducibility)\nSEED = 42\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\nset_seed(SEED)\n\nprint(f\"âœ“ Libraries imported. Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. CONFIGURATION\nDATA_PATH = \"/kaggle/input/mental-health/Combined Data.csv\"\nOUT_DIR = \"distilbert-mental-health\"\nBATCH_SIZE = 16          # DistilBERT is small, so 16 usually fits on P100 GPU\nEPOCHS = 3\nLR = 2e-5                # Standard learning rate for Transformers\nSEED = 42\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\nset_seed(SEED)\n\n\n# Load data from Kaggle dataset \nfilepath = \"/kaggle/input/mental-health/Combined Data.csv\"\ndf = pd.read_csv(filepath)\n\n# Standardize columns\nif \"Unnamed: 0\" in df.columns:\n    df = df.drop(columns=[\"Unnamed: 0\"])\ndf = df.rename(columns={\"statement\": \"text\", \"status\": \"label_name\"})\n\n# Drop Nulls/Duplicates\ndf = df.dropna(subset=[\"text\", \"label_name\"]).drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n\n# ðŸ”¹ Filter to 5 target classes (drop Normal + Personality disorder)\nkeep_labels = [\"Anxiety\", \"Depression\", \"Suicidal\", \"Stress\", \"Bipolar\"]\ndf = df[df[\"label_name\"].isin(keep_labels)].reset_index(drop=True)\n\nprint(\"Updated class distribution:\")\nprint(df[\"label_name\"].value_counts())\n\n# Encode Labels (String -> Integer) AFTER filtering\nlabel_values = sorted(df[\"label_name\"].unique())\nlabel2id = {name: i for i, name in enumerate(label_values)}\nid2label = {i: name for name, i in label2id.items()}\ndf[\"label\"] = df[\"label_name\"].map(label2id)\n\nprint(f\"âœ“ Data Loaded. Shape: {df.shape}\")\nprint(f\"âœ“ Classes: {label2id}\")\n\n\n# Minimal Cleaning Function (Best for Transformers)\ndef minimal_clean(text):\n    text = str(text)\n    text = re.sub(r\"http\\S+|www\\S+|@\\w+\", \"\", text)  # Remove URLs/Mentions\n    text = re.sub(r\"<.*?>\", \"\", text)                 # Remove HTML\n    text = re.sub(r\"\\s+\", \" \", text).strip()          # Remove double spaces\n    return text\n\ndf[\"text\"] = df[\"text\"].apply(minimal_clean)\n\n# 4. SPLIT & TOKENIZE\n# Stratified split ensures equal distribution of classes in Train/Test\ntrain_df, test_df = train_test_split(df, test_size=0.15, stratify=df[\"label\"], random_state=SEED)\n\nhf_train = Dataset.from_pandas(train_df[[\"text\", \"label\"]].reset_index(drop=True))\nhf_test = Dataset.from_pandas(test_df[[\"text\", \"label\"]].reset_index(drop=True))\ndataset = DatasetDict({\"train\": hf_train, \"test\": hf_test})\n\n# 5. COMPUTE CLASS WEIGHTS (Handle Imbalance)\nlabels = train_df[\"label\"].values\nclass_weights = compute_class_weight(\"balanced\", classes=np.unique(labels), y=labels)\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"âš–ï¸ Class Weights calculated: {class_weights}\")\n\n# 6. CUSTOM TRAINER (Fixed for newer Transformers versions)\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        # We add **kwargs above to catch 'num_items_in_batch' or any other new args\n        \n        labels = inputs.get(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        \n        # Calculate loss with our custom class weights\n        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        \n        return (loss, outputs) if return_outputs else loss\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average=\"macro\")\n    return {\"accuracy\": acc, \"f1_macro\": f1}\n\nresults_list = []\nprint(\"âœ… Scoreboard initialized.\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DistilBERT","metadata":{"editable":false}},{"cell_type":"code","source":"FRIENDLY_NAME = \"DistilBERT\"\nMODEL_PATH    = \"distilbert-base-uncased\"\n\nprint(f\"\\n\\n{'='*40}\")\nprint(f\"ðŸ¥Š TRAINING ROUND: {FRIENDLY_NAME}\")\nprint(f\"{'='*40}\")\n\n# 1. Tokenize\nprint(f\">>> Tokenizing for {FRIENDLY_NAME}...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=256)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# 2. Load Model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_PATH,\n    num_labels=len(label2id),\n    id2label=id2label,\n    label2id=label2id\n)\n\n# 3. STAGE 1: WARMUP (Freeze Body)\nprint(f\"â„ï¸ Freezing backbone for warmup...\")\nfor param in model.base_model.parameters():\n    param.requires_grad = False\n    \nwarmup_args = TrainingArguments(\n    output_dir=f\"comparison_{FRIENDLY_NAME}_warmup\",\n    learning_rate=1e-3,\n    num_train_epochs=1,\n    per_device_train_batch_size=16,\n    fp16=torch.cuda.is_available(),\n    report_to=\"none\"\n)\n\nTrainer(\n    model=model, args=warmup_args, \n    train_dataset=tokenized_datasets[\"train\"],\n    data_collator=data_collator\n).train()\n\n# 4. STAGE 2: FINE-TUNE (Unfreeze All)\nprint(f\"ðŸ”¥ Unfreezing for full training...\")\nfor param in model.base_model.parameters():\n    param.requires_grad = True\n    \ntraining_args = TrainingArguments(\n    output_dir=f\"comparison_{FRIENDLY_NAME}_final\",\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1_macro\",\n    fp16=torch.cuda.is_available(),\n    report_to=\"none\"\n)\n\ntrainer = WeightedTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    processing_class=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\n# 5. EVALUATION\nprint(f\"\\nðŸ“Š {FRIENDLY_NAME} Results:\")\n\n# Get Predictions\npreds_output = trainer.predict(tokenized_datasets[\"test\"])\ny_preds = np.argmax(preds_output.predictions, axis=1)\ny_true = preds_output.label_ids\n\n# A. Print Classification Report\nprint(classification_report(y_true, y_preds, target_names=label_values))\n\n# B. Plot Confusion Matrix\nfig, ax = plt.subplots(figsize=(10, 8))\ncm = confusion_matrix(y_true, y_preds, labels=range(len(label_values)))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_values)\ndisp.plot(cmap='Blues', ax=ax, xticks_rotation=45)\nplt.title(f\"Confusion Matrix: {FRIENDLY_NAME}\")\nplt.grid(False)\nplt.show()\n\n# C. Save Score (Only Once!)\nmetrics = trainer.evaluate()\nresults_list.append({\n    \"Model\": FRIENDLY_NAME,\n    \"Accuracy\": metrics[\"eval_accuracy\"],\n    \"F1_Macro\": metrics[\"eval_f1_macro\"]\n})\n\n# Cleanup\ndel model, trainer, tokenizer\ntorch.cuda.empty_cache()\nprint(f\"âœ… {FRIENDLY_NAME} Finished & Cleared from Memory.\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RoBERTa","metadata":{"editable":false}},{"cell_type":"code","source":"FRIENDLY_NAME = \"RoBERTa\"\nMODEL_PATH    = \"roberta-base\"\n\nprint(f\"\\n\\n{'='*40}\")\nprint(f\"ðŸ¥Š TRAINING ROUND: {FRIENDLY_NAME}\")\nprint(f\"{'='*40}\")\n\n# 1. Tokenize\nprint(f\">>> Tokenizing for {FRIENDLY_NAME}...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=256)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# 2. Load Model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_PATH,\n    num_labels=len(label2id),\n    id2label=id2label,\n    label2id=label2id\n)\n\n# 3. STAGE 1: WARMUP (Freeze Body)\nprint(f\"â„ï¸ Freezing backbone for warmup...\")\nfor param in model.base_model.parameters():\n    param.requires_grad = False\n    \nwarmup_args = TrainingArguments(\n    output_dir=f\"comparison_{FRIENDLY_NAME}_warmup\",\n    learning_rate=1e-3,\n    num_train_epochs=1,\n    per_device_train_batch_size=16,\n    fp16=torch.cuda.is_available(),\n    report_to=\"none\"\n)\n\nTrainer(\n    model=model, args=warmup_args, \n    train_dataset=tokenized_datasets[\"train\"],\n    data_collator=data_collator\n).train()\n\n# 4. STAGE 2: FINE-TUNE (Unfreeze All)\nprint(f\"ðŸ”¥ Unfreezing for full training...\")\nfor param in model.base_model.parameters():\n    param.requires_grad = True\n    \ntraining_args = TrainingArguments(\n    output_dir=f\"comparison_{FRIENDLY_NAME}_final\",\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1_macro\",\n    fp16=torch.cuda.is_available(),\n    report_to=\"none\"\n)\n\ntrainer = WeightedTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    processing_class=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\n# 5. EVALUATION\nprint(f\"\\nðŸ“Š {FRIENDLY_NAME} Results:\")\n\n# Get Predictions\npreds_output = trainer.predict(tokenized_datasets[\"test\"])\ny_preds = np.argmax(preds_output.predictions, axis=1)\ny_true = preds_output.label_ids\n\n# A. Print Classification Report\nprint(classification_report(y_true, y_preds, target_names=label_values))\n\n# B. Plot Confusion Matrix\nfig, ax = plt.subplots(figsize=(10, 8))\ncm = confusion_matrix(y_true, y_preds, labels=range(len(label_values)))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_values)\ndisp.plot(cmap='Blues', ax=ax, xticks_rotation=45)\nplt.title(f\"Confusion Matrix: {FRIENDLY_NAME}\")\nplt.grid(False)\nplt.show()\n\n# C. Save Score (Only Once!)\nmetrics = trainer.evaluate()\nresults_list.append({\n    \"Model\": FRIENDLY_NAME,\n    \"Accuracy\": metrics[\"eval_accuracy\"],\n    \"F1_Macro\": metrics[\"eval_f1_macro\"]\n})\n\n# Cleanup\ndel model, trainer, tokenizer\ntorch.cuda.empty_cache()\nprint(f\"âœ… {FRIENDLY_NAME} Finished & Cleared from Memory.\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BioBERT","metadata":{"editable":false}},{"cell_type":"code","source":"FRIENDLY_NAME = \"BioBERT\"\nMODEL_PATH    = \"dmis-lab/biobert-v1.1\"\n\nprint(f\"\\n\\n{'='*40}\")\nprint(f\"ðŸ¥Š TRAINING ROUND: {FRIENDLY_NAME}\")\nprint(f\"{'='*40}\")\n\n# 1. Tokenize\nprint(f\">>> Tokenizing for {FRIENDLY_NAME}...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=256)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# 2. Load Model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_PATH,\n    num_labels=len(label2id),\n    id2label=id2label,\n    label2id=label2id\n)\n\n# 3. STAGE 1: WARMUP (Freeze Body)\nprint(f\"â„ï¸ Freezing backbone for warmup...\")\nfor param in model.base_model.parameters():\n    param.requires_grad = False\n    \nwarmup_args = TrainingArguments(\n    output_dir=f\"comparison_{FRIENDLY_NAME}_warmup\",\n    learning_rate=1e-3,\n    num_train_epochs=1,\n    per_device_train_batch_size=16,\n    fp16=torch.cuda.is_available(),\n    report_to=\"none\"\n)\n\nTrainer(\n    model=model, args=warmup_args, \n    train_dataset=tokenized_datasets[\"train\"],\n    data_collator=data_collator\n).train()\n\n# 4. STAGE 2: FINE-TUNE (Unfreeze All)\nprint(f\"ðŸ”¥ Unfreezing for full training...\")\nfor param in model.base_model.parameters():\n    param.requires_grad = True\n    \ntraining_args = TrainingArguments(\n    output_dir=f\"comparison_{FRIENDLY_NAME}_final\",\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1_macro\",\n    fp16=torch.cuda.is_available(),\n    report_to=\"none\"\n)\n\ntrainer = WeightedTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    processing_class=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\n# 5. EVALUATION\nprint(f\"\\nðŸ“Š {FRIENDLY_NAME} Results:\")\n\n# Get Predictions\npreds_output = trainer.predict(tokenized_datasets[\"test\"])\ny_preds = np.argmax(preds_output.predictions, axis=1)\ny_true = preds_output.label_ids\n\n# A. Print Classification Report\nprint(classification_report(y_true, y_preds, target_names=label_values))\n\n# B. Plot Confusion Matrix\nfig, ax = plt.subplots(figsize=(10, 8))\ncm = confusion_matrix(y_true, y_preds, labels=range(len(label_values)))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_values)\ndisp.plot(cmap='Blues', ax=ax, xticks_rotation=45)\nplt.title(f\"Confusion Matrix: {FRIENDLY_NAME}\")\nplt.grid(False)\nplt.show()\n\n# C. Save Score (Only Once!)\nmetrics = trainer.evaluate()\nresults_list.append({\n    \"Model\": FRIENDLY_NAME,\n    \"Accuracy\": metrics[\"eval_accuracy\"],\n    \"F1_Macro\": metrics[\"eval_f1_macro\"]\n})\n\n# Cleanup\ndel model, trainer, tokenizer\ntorch.cuda.empty_cache()\nprint(f\"âœ… {FRIENDLY_NAME} Finished & Cleared from Memory.\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PubMedBERT","metadata":{"editable":false}},{"cell_type":"code","source":"FRIENDLY_NAME = \"PubMedBERT\"\nMODEL_PATH    = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n\nprint(f\"\\n\\n{'='*40}\")\nprint(f\"ðŸ¥Š TRAINING ROUND: {FRIENDLY_NAME}\")\nprint(f\"{'='*40}\")\n\n# 1. Tokenize\nprint(f\">>> Tokenizing for {FRIENDLY_NAME}...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=256)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# 2. Load Model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_PATH,\n    num_labels=len(label2id),\n    id2label=id2label,\n    label2id=label2id\n)\n\n# 3. STAGE 1: WARMUP (Freeze Body)\nprint(f\"â„ï¸ Freezing backbone for warmup...\")\nfor param in model.base_model.parameters():\n    param.requires_grad = False\n    \nwarmup_args = TrainingArguments(\n    output_dir=f\"comparison_{FRIENDLY_NAME}_warmup\",\n    learning_rate=1e-3,\n    num_train_epochs=1,\n    per_device_train_batch_size=16,\n    fp16=torch.cuda.is_available(),\n    report_to=\"none\"\n)\n\nTrainer(\n    model=model, args=warmup_args, \n    train_dataset=tokenized_datasets[\"train\"],\n    data_collator=data_collator\n).train()\n\n# 4. STAGE 2: FINE-TUNE (Unfreeze All)\nprint(f\"ðŸ”¥ Unfreezing for full training...\")\nfor param in model.base_model.parameters():\n    param.requires_grad = True\n    \ntraining_args = TrainingArguments(\n    output_dir=f\"comparison_{FRIENDLY_NAME}_final\",\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1_macro\",\n    fp16=torch.cuda.is_available(),\n    report_to=\"none\"\n)\n\ntrainer = WeightedTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    processing_class=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\n# 5. EVALUATION\nprint(f\"\\nðŸ“Š {FRIENDLY_NAME} Results:\")\n\n# Get Predictions\npreds_output = trainer.predict(tokenized_datasets[\"test\"])\ny_preds = np.argmax(preds_output.predictions, axis=1)\ny_true = preds_output.label_ids\n\n# A. Print Classification Report\nprint(classification_report(y_true, y_preds, target_names=label_values))\n\n# B. Plot Confusion Matrix\nfig, ax = plt.subplots(figsize=(10, 8))\ncm = confusion_matrix(y_true, y_preds, labels=range(len(label_values)))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_values)\ndisp.plot(cmap='Blues', ax=ax, xticks_rotation=45)\nplt.title(f\"Confusion Matrix: {FRIENDLY_NAME}\")\nplt.grid(False)\nplt.show()\n\n# C. Save Score (Only Once!)\nmetrics = trainer.evaluate()\nresults_list.append({\n    \"Model\": FRIENDLY_NAME,\n    \"Accuracy\": metrics[\"eval_accuracy\"],\n    \"F1_Macro\": metrics[\"eval_f1_macro\"]\n})\n\n# Cleanup\ndel model, trainer, tokenizer\ntorch.cuda.empty_cache()\nprint(f\"âœ… {FRIENDLY_NAME} Finished & Cleared from Memory.\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ClinicalBERT","metadata":{"editable":false}},{"cell_type":"code","source":"FRIENDLY_NAME = \"ClinicalBERT\"\nMODEL_PATH    = \"emilyalsentzer/Bio_ClinicalBERT\"\n\nprint(f\"\\n\\n{'='*40}\")\nprint(f\"ðŸ¥Š TRAINING ROUND: {FRIENDLY_NAME}\")\nprint(f\"{'='*40}\")\n\n# 1. Tokenize\nprint(f\">>> Tokenizing for {FRIENDLY_NAME}...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=256)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# 2. Load Model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_PATH,\n    num_labels=len(label2id),\n    id2label=id2label,\n    label2id=label2id\n)\n\n# 3. STAGE 1: WARMUP (Freeze Body)\nprint(f\"â„ï¸ Freezing backbone for warmup...\")\nfor param in model.base_model.parameters():\n    param.requires_grad = False\n    \nwarmup_args = TrainingArguments(\n    output_dir=f\"comparison_{FRIENDLY_NAME}_warmup\",\n    learning_rate=1e-3,\n    num_train_epochs=1,\n    per_device_train_batch_size=16,\n    fp16=torch.cuda.is_available(),\n    report_to=\"none\"\n)\n\nTrainer(\n    model=model, args=warmup_args, \n    train_dataset=tokenized_datasets[\"train\"],\n    data_collator=data_collator\n).train()\n\n# 4. STAGE 2: FINE-TUNE (Unfreeze All)\nprint(f\"ðŸ”¥ Unfreezing for full training...\")\nfor param in model.base_model.parameters():\n    param.requires_grad = True\n    \ntraining_args = TrainingArguments(\n    output_dir=f\"comparison_{FRIENDLY_NAME}_final\",\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1_macro\",\n    fp16=torch.cuda.is_available(),\n    report_to=\"none\"\n)\n\ntrainer = WeightedTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    processing_class=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\n# 5. EVALUATION\nprint(f\"\\nðŸ“Š {FRIENDLY_NAME} Results:\")\n\n# Get Predictions\npreds_output = trainer.predict(tokenized_datasets[\"test\"])\ny_preds = np.argmax(preds_output.predictions, axis=1)\ny_true = preds_output.label_ids\n\n# A. Print Classification Report\nprint(classification_report(y_true, y_preds, target_names=label_values))\n\n# B. Plot Confusion Matrix\nfig, ax = plt.subplots(figsize=(10, 8))\ncm = confusion_matrix(y_true, y_preds, labels=range(len(label_values)))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_values)\ndisp.plot(cmap='Blues', ax=ax, xticks_rotation=45)\nplt.title(f\"Confusion Matrix: {FRIENDLY_NAME}\")\nplt.grid(False)\nplt.show()\n\n# C. Save Score (Only Once!)\nmetrics = trainer.evaluate()\nresults_list.append({\n    \"Model\": FRIENDLY_NAME,\n    \"Accuracy\": metrics[\"eval_accuracy\"],\n    \"F1_Macro\": metrics[\"eval_f1_macro\"]\n})\n\n# Cleanup\ndel model, trainer, tokenizer\ntorch.cuda.empty_cache()\nprint(f\"âœ… {FRIENDLY_NAME} Finished & Cleared from Memory.\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize the Battle\nprint(\"\\nðŸ“Š FINAL RESULTS TABLE:\")\ndf_results = pd.DataFrame(results_list)\nprint(df_results)\n\n# Plot\nplt.figure(figsize=(10, 6))\nsns.barplot(data=df_results, x=\"Model\", y=\"F1_Macro\", palette=\"viridis\")\nplt.title(\"Transformer Model Comparison (Macro F1 Score)\")\nplt.ylabel(\"F1 Score\")\nplt.ylim(0.7, 1.0) \n\n# Add labels\nfor index, row in df_results.iterrows():\n    plt.text(index, row.F1_Macro, f\"{row.F1_Macro:.4f}\", color='black', ha=\"center\", va='bottom')\n\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 8. EVALUATION & SAVING\n# ----------------------\nprint(\"\\nðŸ“Š Final Evaluation:\")\npreds_output = trainer.predict(tokenized_datasets[\"test\"])\ny_preds = np.argmax(preds_output.predictions, axis=1)\ny_true = preds_output.label_ids\n\nprint(classification_report(y_true, y_preds, target_names=label_values))\n\n# Save Model\nprint(f\">>> Saving model to {OUT_DIR}...\")\ntrainer.save_model(OUT_DIR)\ntokenizer.save_pretrained(OUT_DIR)\n\n# 9. INTERACTIVE TEST\n# -------------------\nprint(\"\\nðŸ§ª Quick Test:\")\nclassifier = pipeline(\"text-classification\", model=OUT_DIR, device=0 if torch.cuda.is_available() else -1)\ntext = \"I feel really anxious and I cannot sleep at night.\"\nresult = classifier(text)\nprint(f\"Input: {text}\")\nprint(f\"Prediction: {result}\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}