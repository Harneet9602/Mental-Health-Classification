{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9522b13-7ae5-435e-8d89-38e4c68fdf3d",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T04:55:22.574823Z",
     "iopub.status.busy": "2025-11-23T04:55:22.574441Z",
     "iopub.status.idle": "2025-11-23T04:57:31.701090Z",
     "shell.execute_reply": "2025-11-23T04:57:31.700025Z"
    },
    "papermill": {
     "duration": 129.134946,
     "end_time": "2025-11-23T04:57:31.703224",
     "exception": false,
     "start_time": "2025-11-23T04:55:22.568278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\"\"\"\n",
    "CAPSTONE PROJECT: MULTI-CLASS MENTAL HEALTH CLASSIFICATION\n",
    "Complete notebook with preprocessing, data loading, and all 6 transformer models\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e1a852-f5fb-445d-91dd-fc0a4fed1263",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T04:55:22.574823Z",
     "iopub.status.busy": "2025-11-23T04:55:22.574441Z",
     "iopub.status.idle": "2025-11-23T04:57:31.701090Z",
     "shell.execute_reply": "2025-11-23T04:57:31.700025Z"
    },
    "papermill": {
     "duration": 129.134946,
     "end_time": "2025-11-23T04:57:31.703224",
     "exception": false,
     "start_time": "2025-11-23T04:55:22.568278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# CELL 0: INSTALL REQUIRED PACKAGES\n",
    "# ===============================================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages with compatible versions for Kaggle\"\"\"\n",
    "    \n",
    "    packages_to_install = [\n",
    "        (\"numpy\", \"numpy<2.0\"),\n",
    "        (\"scikit-learn\", \"scikit-learn>=1.0.0\"),\n",
    "        (\"pandas\", \"pandas\"),\n",
    "        (\"matplotlib\", \"matplotlib\"),\n",
    "        (\"seaborn\", \"seaborn\"),\n",
    "        (\"nltk\", \"nltk\"),\n",
    "        (\"torch\", \"torch\"),\n",
    "        (\"transformers\", \"transformers\"),\n",
    "        (\"datasets\", \"datasets\")\n",
    "    ]\n",
    "    \n",
    "    installed_count = 0\n",
    "    for package_name, package_spec in packages_to_install:\n",
    "        try:\n",
    "            subprocess.check_call(\n",
    "                [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package_spec],\n",
    "                stderr=subprocess.DEVNULL\n",
    "            )\n",
    "            print(f\"‚úì {package_name}\")\n",
    "            installed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† {package_name} - {str(e)[:50]}\")\n",
    "    \n",
    "    print(f\"\\n‚úì Installation complete! {installed_count}/{len(packages_to_install)} packages ready.\")\n",
    "    print(\"‚ö† Note: Some Kaggle pre-existing conflicts are normal and safe to ignore.\\n\")\n",
    "\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c632af",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T04:57:31.715827Z",
     "iopub.status.busy": "2025-11-23T04:57:31.715504Z",
     "iopub.status.idle": "2025-11-23T04:57:31.720263Z",
     "shell.execute_reply": "2025-11-23T04:57:31.719407Z"
    },
    "papermill": {
     "duration": 0.012225,
     "end_time": "2025-11-23T04:57:31.721555",
     "exception": false,
     "start_time": "2025-11-23T04:57:31.709330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ===============================================================================\n",
    "# # FIX CUDNN COMPATIBILITY (KAGGLE P100)\n",
    "# # ===============================================================================\n",
    "\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# # Disable cuDNN to avoid version mismatch on Kaggle\n",
    "# tf.config.list_physical_devices('GPU')\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# # Re-enable GPU with memory growth to prevent OOM\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# for gpu in gpus:\n",
    "#     try:\n",
    "#         tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     except RuntimeError as e:\n",
    "#         print(f\"GPU setup warning (safe to ignore): {e}\")\n",
    "\n",
    "# print(\"‚úì GPU configured and cuDNN compatibility fixed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c72345c5",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T04:57:31.734149Z",
     "iopub.status.busy": "2025-11-23T04:57:31.733794Z",
     "iopub.status.idle": "2025-11-23T04:58:23.630954Z",
     "shell.execute_reply": "2025-11-23T04:58:23.629992Z"
    },
    "papermill": {
     "duration": 51.904722,
     "end_time": "2025-11-23T04:58:23.632600",
     "exception": false,
     "start_time": "2025-11-23T04:57:31.727878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 04:57:54.824329: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763873875.070451      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763873875.139092      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported. Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# CELL 1: CLEAN IMPORTS & SETUP\n",
    "# ===============================================================================\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Hugging Face & Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Random Seed (For Reproducibility)\n",
    "SEED = 42\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(SEED)\n",
    "\n",
    "print(f\"‚úì Libraries imported. Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b88c78",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T04:58:23.645390Z",
     "iopub.status.busy": "2025-11-23T04:58:23.644599Z",
     "iopub.status.idle": "2025-11-23T04:58:27.520086Z",
     "shell.execute_reply": "2025-11-23T04:58:27.518821Z"
    },
    "papermill": {
     "duration": 3.884313,
     "end_time": "2025-11-23T04:58:27.521935",
     "exception": false,
     "start_time": "2025-11-23T04:58:23.637622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated class distribution:\n",
      "label_name\n",
      "Depression    15087\n",
      "Suicidal      10641\n",
      "Anxiety        3617\n",
      "Bipolar        2501\n",
      "Stress         2293\n",
      "Name: count, dtype: int64\n",
      "‚úì Data Loaded. Shape: (34139, 3)\n",
      "‚úì Classes: {'Anxiety': 0, 'Bipolar': 1, 'Depression': 2, 'Stress': 3, 'Suicidal': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Class Weights calculated: tensor([1.8880, 2.7298, 0.4526, 2.9777, 0.6416])\n",
      "‚úÖ Scoreboard initialized.\n"
     ]
    }
   ],
   "source": [
    "# 2. CONFIGURATION\n",
    "DATA_PATH = \"/kaggle/input/mental-health/Combined Data.csv\"\n",
    "OUT_DIR = \"distilbert-mental-health\"\n",
    "BATCH_SIZE = 16          # DistilBERT is small, so 16 usually fits on P100 GPU\n",
    "EPOCHS = 3\n",
    "LR = 2e-5                # Standard learning rate for Transformers\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(SEED)\n",
    "\n",
    "\n",
    "# Load data from Kaggle dataset \n",
    "filepath = \"/kaggle/input/mental-health/Combined Data.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Standardize columns\n",
    "if \"Unnamed: 0\" in df.columns:\n",
    "    df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "df = df.rename(columns={\"statement\": \"text\", \"status\": \"label_name\"})\n",
    "\n",
    "# Drop Nulls/Duplicates\n",
    "df = df.dropna(subset=[\"text\", \"label_name\"]).drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n",
    "\n",
    "# üîπ Filter to 5 target classes (drop Normal + Personality disorder)\n",
    "keep_labels = [\"Anxiety\", \"Depression\", \"Suicidal\", \"Stress\", \"Bipolar\"]\n",
    "df = df[df[\"label_name\"].isin(keep_labels)].reset_index(drop=True)\n",
    "\n",
    "print(\"Updated class distribution:\")\n",
    "print(df[\"label_name\"].value_counts())\n",
    "\n",
    "# Encode Labels (String -> Integer) AFTER filtering\n",
    "label_values = sorted(df[\"label_name\"].unique())\n",
    "label2id = {name: i for i, name in enumerate(label_values)}\n",
    "id2label = {i: name for name, i in label2id.items()}\n",
    "df[\"label\"] = df[\"label_name\"].map(label2id)\n",
    "\n",
    "print(f\"‚úì Data Loaded. Shape: {df.shape}\")\n",
    "print(f\"‚úì Classes: {label2id}\")\n",
    "\n",
    "\n",
    "# Minimal Cleaning Function (Best for Transformers)\n",
    "def minimal_clean(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|@\\w+\", \"\", text)  # Remove URLs/Mentions\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)                 # Remove HTML\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()          # Remove double spaces\n",
    "    return text\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(minimal_clean)\n",
    "\n",
    "# 4. SPLIT & TOKENIZE\n",
    "# Stratified split ensures equal distribution of classes in Train/Test\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, stratify=df[\"label\"], random_state=SEED)\n",
    "\n",
    "hf_train = Dataset.from_pandas(train_df[[\"text\", \"label\"]].reset_index(drop=True))\n",
    "hf_test = Dataset.from_pandas(test_df[[\"text\", \"label\"]].reset_index(drop=True))\n",
    "dataset = DatasetDict({\"train\": hf_train, \"test\": hf_test})\n",
    "\n",
    "# 5. COMPUTE CLASS WEIGHTS (Handle Imbalance)\n",
    "labels = train_df[\"label\"].values\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"‚öñÔ∏è Class Weights calculated: {class_weights}\")\n",
    "\n",
    "# 6. CUSTOM TRAINER (Fixed for newer Transformers versions)\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # We add **kwargs above to catch 'num_items_in_batch' or any other new args\n",
    "        \n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Calculate loss with our custom class weights\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
    "\n",
    "results_list = []\n",
    "print(\"‚úÖ Scoreboard initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774f321",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": 0.005065,
     "end_time": "2025-11-23T04:58:27.532310",
     "exception": false,
     "start_time": "2025-11-23T04:58:27.527245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec7f403",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T04:58:27.544216Z",
     "iopub.status.busy": "2025-11-23T04:58:27.543367Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-11-23T04:58:27.537160",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================\n",
      "ü•ä TRAINING ROUND: DistilBERT\n",
      "========================================\n",
      ">>> Tokenizing for DistilBERT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290ee2a5d2de408884ebd773eea20942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f870d8286f4bdbb1ff7f4bdfc29c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4239ef076a40441495bafe2a5b1e602d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252653d0e9e84ce6bbb6b67c2e9a8f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee78e7b0003d484b8784d4e9ea3228e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29018 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ae34407d344debb493f1dda1b6e2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5121 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d859255b9346b8894e5a390cc8049d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùÑÔ∏è Freezing backbone for warmup...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1814' max='1814' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1814/1814 2:22:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.996800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.857300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.788000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Unfreezing for full training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3009' max='5442' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3009/5442 9:33:22 < 7:43:55, 0.09 it/s, Epoch 1.66/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.496100</td>\n",
       "      <td>0.469422</td>\n",
       "      <td>0.773482</td>\n",
       "      <td>0.802431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FRIENDLY_NAME = \"DistilBERT\"\n",
    "MODEL_PATH    = \"distilbert-base-uncased\"\n",
    "\n",
    "print(f\"\\n\\n{'='*40}\")\n",
    "print(f\"ü•ä TRAINING ROUND: {FRIENDLY_NAME}\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "# 1. Tokenize\n",
    "print(f\">>> Tokenizing for {FRIENDLY_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=256)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 2. Load Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# 3. STAGE 1: WARMUP (Freeze Body)\n",
    "print(f\"‚ùÑÔ∏è Freezing backbone for warmup...\")\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "warmup_args = TrainingArguments(\n",
    "    output_dir=f\"comparison_{FRIENDLY_NAME}_warmup\",\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "Trainer(\n",
    "    model=model, args=warmup_args, \n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator\n",
    ").train()\n",
    "\n",
    "# 4. STAGE 2: FINE-TUNE (Unfreeze All)\n",
    "print(f\"üî• Unfreezing for full training...\")\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"comparison_{FRIENDLY_NAME}_final\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 5. EVALUATION\n",
    "print(f\"\\nüìä {FRIENDLY_NAME} Results:\")\n",
    "\n",
    "# Get Predictions\n",
    "preds_output = trainer.predict(tokenized_datasets[\"test\"])\n",
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "y_true = preds_output.label_ids\n",
    "\n",
    "# A. Print Classification Report\n",
    "print(classification_report(y_true, y_preds, target_names=label_values))\n",
    "\n",
    "# B. Plot Confusion Matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_true, y_preds, labels=range(len(label_values)))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_values)\n",
    "disp.plot(cmap='Blues', ax=ax, xticks_rotation=45)\n",
    "plt.title(f\"Confusion Matrix: {FRIENDLY_NAME}\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# C. Save Score (Only Once!)\n",
    "metrics = trainer.evaluate()\n",
    "results_list.append({\n",
    "    \"Model\": FRIENDLY_NAME,\n",
    "    \"Accuracy\": metrics[\"eval_accuracy\"],\n",
    "    \"F1_Macro\": metrics[\"eval_f1_macro\"]\n",
    "})\n",
    "\n",
    "# Cleanup\n",
    "del model, trainer, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"‚úÖ {FRIENDLY_NAME} Finished & Cleared from Memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c27126f",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f31c2b",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FRIENDLY_NAME = \"RoBERTa\"\n",
    "MODEL_PATH    = \"roberta-base\"\n",
    "\n",
    "print(f\"\\n\\n{'='*40}\")\n",
    "print(f\"ü•ä TRAINING ROUND: {FRIENDLY_NAME}\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "# 1. Tokenize\n",
    "print(f\">>> Tokenizing for {FRIENDLY_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=256)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 2. Load Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# 3. STAGE 1: WARMUP (Freeze Body)\n",
    "print(f\"‚ùÑÔ∏è Freezing backbone for warmup...\")\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "warmup_args = TrainingArguments(\n",
    "    output_dir=f\"comparison_{FRIENDLY_NAME}_warmup\",\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "Trainer(\n",
    "    model=model, args=warmup_args, \n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator\n",
    ").train()\n",
    "\n",
    "# 4. STAGE 2: FINE-TUNE (Unfreeze All)\n",
    "print(f\"üî• Unfreezing for full training...\")\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"comparison_{FRIENDLY_NAME}_final\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 5. EVALUATION\n",
    "print(f\"\\nüìä {FRIENDLY_NAME} Results:\")\n",
    "\n",
    "# Get Predictions\n",
    "preds_output = trainer.predict(tokenized_datasets[\"test\"])\n",
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "y_true = preds_output.label_ids\n",
    "\n",
    "# A. Print Classification Report\n",
    "print(classification_report(y_true, y_preds, target_names=label_values))\n",
    "\n",
    "# B. Plot Confusion Matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_true, y_preds, labels=range(len(label_values)))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_values)\n",
    "disp.plot(cmap='Blues', ax=ax, xticks_rotation=45)\n",
    "plt.title(f\"Confusion Matrix: {FRIENDLY_NAME}\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# C. Save Score (Only Once!)\n",
    "metrics = trainer.evaluate()\n",
    "results_list.append({\n",
    "    \"Model\": FRIENDLY_NAME,\n",
    "    \"Accuracy\": metrics[\"eval_accuracy\"],\n",
    "    \"F1_Macro\": metrics[\"eval_f1_macro\"]\n",
    "})\n",
    "\n",
    "# Cleanup\n",
    "del model, trainer, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"‚úÖ {FRIENDLY_NAME} Finished & Cleared from Memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40bda4",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# BioBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd95e4",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FRIENDLY_NAME = \"BioBERT\"\n",
    "MODEL_PATH    = \"dmis-lab/biobert-v1.1\"\n",
    "\n",
    "print(f\"\\n\\n{'='*40}\")\n",
    "print(f\"ü•ä TRAINING ROUND: {FRIENDLY_NAME}\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "# 1. Tokenize\n",
    "print(f\">>> Tokenizing for {FRIENDLY_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=256)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 2. Load Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# 3. STAGE 1: WARMUP (Freeze Body)\n",
    "print(f\"‚ùÑÔ∏è Freezing backbone for warmup...\")\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "warmup_args = TrainingArguments(\n",
    "    output_dir=f\"comparison_{FRIENDLY_NAME}_warmup\",\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "Trainer(\n",
    "    model=model, args=warmup_args, \n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator\n",
    ").train()\n",
    "\n",
    "# 4. STAGE 2: FINE-TUNE (Unfreeze All)\n",
    "print(f\"üî• Unfreezing for full training...\")\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"comparison_{FRIENDLY_NAME}_final\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 5. EVALUATION\n",
    "print(f\"\\nüìä {FRIENDLY_NAME} Results:\")\n",
    "\n",
    "# Get Predictions\n",
    "preds_output = trainer.predict(tokenized_datasets[\"test\"])\n",
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "y_true = preds_output.label_ids\n",
    "\n",
    "# A. Print Classification Report\n",
    "print(classification_report(y_true, y_preds, target_names=label_values))\n",
    "\n",
    "# B. Plot Confusion Matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_true, y_preds, labels=range(len(label_values)))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_values)\n",
    "disp.plot(cmap='Blues', ax=ax, xticks_rotation=45)\n",
    "plt.title(f\"Confusion Matrix: {FRIENDLY_NAME}\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# C. Save Score (Only Once!)\n",
    "metrics = trainer.evaluate()\n",
    "results_list.append({\n",
    "    \"Model\": FRIENDLY_NAME,\n",
    "    \"Accuracy\": metrics[\"eval_accuracy\"],\n",
    "    \"F1_Macro\": metrics[\"eval_f1_macro\"]\n",
    "})\n",
    "\n",
    "# Cleanup\n",
    "del model, trainer, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"‚úÖ {FRIENDLY_NAME} Finished & Cleared from Memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902bfd58",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# PubMedBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47870d96",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FRIENDLY_NAME = \"PubMedBERT\"\n",
    "MODEL_PATH    = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "\n",
    "print(f\"\\n\\n{'='*40}\")\n",
    "print(f\"ü•ä TRAINING ROUND: {FRIENDLY_NAME}\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "# 1. Tokenize\n",
    "print(f\">>> Tokenizing for {FRIENDLY_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=256)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 2. Load Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# 3. STAGE 1: WARMUP (Freeze Body)\n",
    "print(f\"‚ùÑÔ∏è Freezing backbone for warmup...\")\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "warmup_args = TrainingArguments(\n",
    "    output_dir=f\"comparison_{FRIENDLY_NAME}_warmup\",\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "Trainer(\n",
    "    model=model, args=warmup_args, \n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator\n",
    ").train()\n",
    "\n",
    "# 4. STAGE 2: FINE-TUNE (Unfreeze All)\n",
    "print(f\"üî• Unfreezing for full training...\")\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"comparison_{FRIENDLY_NAME}_final\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 5. EVALUATION\n",
    "print(f\"\\nüìä {FRIENDLY_NAME} Results:\")\n",
    "\n",
    "# Get Predictions\n",
    "preds_output = trainer.predict(tokenized_datasets[\"test\"])\n",
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "y_true = preds_output.label_ids\n",
    "\n",
    "# A. Print Classification Report\n",
    "print(classification_report(y_true, y_preds, target_names=label_values))\n",
    "\n",
    "# B. Plot Confusion Matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_true, y_preds, labels=range(len(label_values)))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_values)\n",
    "disp.plot(cmap='Blues', ax=ax, xticks_rotation=45)\n",
    "plt.title(f\"Confusion Matrix: {FRIENDLY_NAME}\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# C. Save Score (Only Once!)\n",
    "metrics = trainer.evaluate()\n",
    "results_list.append({\n",
    "    \"Model\": FRIENDLY_NAME,\n",
    "    \"Accuracy\": metrics[\"eval_accuracy\"],\n",
    "    \"F1_Macro\": metrics[\"eval_f1_macro\"]\n",
    "})\n",
    "\n",
    "# Cleanup\n",
    "del model, trainer, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"‚úÖ {FRIENDLY_NAME} Finished & Cleared from Memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aaa983",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# ClinicalBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e8b61",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FRIENDLY_NAME = \"ClinicalBERT\"\n",
    "MODEL_PATH    = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "\n",
    "print(f\"\\n\\n{'='*40}\")\n",
    "print(f\"ü•ä TRAINING ROUND: {FRIENDLY_NAME}\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "# 1. Tokenize\n",
    "print(f\">>> Tokenizing for {FRIENDLY_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=256)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 2. Load Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# 3. STAGE 1: WARMUP (Freeze Body)\n",
    "print(f\"‚ùÑÔ∏è Freezing backbone for warmup...\")\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "warmup_args = TrainingArguments(\n",
    "    output_dir=f\"comparison_{FRIENDLY_NAME}_warmup\",\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "Trainer(\n",
    "    model=model, args=warmup_args, \n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator\n",
    ").train()\n",
    "\n",
    "# 4. STAGE 2: FINE-TUNE (Unfreeze All)\n",
    "print(f\"üî• Unfreezing for full training...\")\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"comparison_{FRIENDLY_NAME}_final\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 5. EVALUATION\n",
    "print(f\"\\nüìä {FRIENDLY_NAME} Results:\")\n",
    "\n",
    "# Get Predictions\n",
    "preds_output = trainer.predict(tokenized_datasets[\"test\"])\n",
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "y_true = preds_output.label_ids\n",
    "\n",
    "# A. Print Classification Report\n",
    "print(classification_report(y_true, y_preds, target_names=label_values))\n",
    "\n",
    "# B. Plot Confusion Matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_true, y_preds, labels=range(len(label_values)))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_values)\n",
    "disp.plot(cmap='Blues', ax=ax, xticks_rotation=45)\n",
    "plt.title(f\"Confusion Matrix: {FRIENDLY_NAME}\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# C. Save Score (Only Once!)\n",
    "metrics = trainer.evaluate()\n",
    "results_list.append({\n",
    "    \"Model\": FRIENDLY_NAME,\n",
    "    \"Accuracy\": metrics[\"eval_accuracy\"],\n",
    "    \"F1_Macro\": metrics[\"eval_f1_macro\"]\n",
    "})\n",
    "\n",
    "# Cleanup\n",
    "del model, trainer, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"‚úÖ {FRIENDLY_NAME} Finished & Cleared from Memory.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6426913,
     "sourceId": 10375532,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 2821,
     "modelInstanceId": 4689,
     "sourceId": 6068,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-23T04:55:17.791175",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
